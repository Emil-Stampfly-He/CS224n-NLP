{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0b7629-42c7-4049-bbc3-28d1ee14118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd6be5a-b8b9-4219-b31b-92601a8167bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(embedding=True, forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1589177-07c5-49ca-945b-c54c3ae3a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_lookup sanity check passes!\n"
     ]
    }
   ],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n",
    "        \n",
    "        # Initializing weights and biases in embedding-to-hidden layer\n",
    "        self.embed_to_hidden_weight = nn.Parameter(torch.zeros(self.embed_size*n_features, hidden_size))\n",
    "        self.embed_to_hidden_bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        init.xavier_uniform_(self.embed_to_hidden_weight)\n",
    "        init.uniform_(self.embed_to_hidden_bias)\n",
    "        \n",
    "        # Initializing dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        # Initializing weights and biases in hidden-to-output layer\n",
    "        self.hidden_to_logits_weight = nn.Parameter(torch.zeros(hidden_size, n_classes))\n",
    "        self.hidden_to_logits_bias = nn.Parameter(torch.zeros(n_classes))\n",
    "        init.xavier_uniform_(self.hidden_to_logits_weight)\n",
    "        init.uniform_(self.hidden_to_logits_bias)\n",
    "        \n",
    "\n",
    "    def embedding_lookup(self, w):\n",
    "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
    "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in w\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        x = self.embeddings[w]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, w):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n",
    "                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(w) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        # Get an x\n",
    "        x = self.embedding_lookup(w)\n",
    "        \n",
    "        # h = ReLU(xW + b_1)\n",
    "        x = torch.matmul(x, self.embed_to_hidden_weight) + self.embed_to_hidden_bias\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Output logits\n",
    "        logits = torch.matmul(x, self.hidden_to_logits_weight) + self.hidden_to_logits_bias\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
    "    parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
    "    parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "    model = ParserModel(embeddings)\n",
    "\n",
    "    def check_embedding():\n",
    "        inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        selected = model.embedding_lookup(inds)\n",
    "        assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
    "                                      + repr(selected) + \" contains non-zero elements.\"\n",
    "\n",
    "    def check_forward():\n",
    "        inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        out = model(inputs)\n",
    "        expected_out_shape = (4, 3)\n",
    "        assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
    "                                                \" which doesn't match expected \" + repr(expected_out_shape)\n",
    "\n",
    "    if args.embedding:\n",
    "        check_embedding()\n",
    "        print(\"Embedding_lookup sanity check passes!\")\n",
    "\n",
    "    if args.forward:\n",
    "        check_forward()\n",
    "        print(\"Forward sanity check passes!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "478118e0-f72f-4160-a243-b285861f0ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_lookup sanity check passes!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
    "    parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
    "    parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "    model = ParserModel(embeddings)\n",
    "\n",
    "    def check_embedding():\n",
    "        inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        selected = model.embedding_lookup(inds)\n",
    "        assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
    "                                      + repr(selected) + \" contains non-zero elements.\"\n",
    "    if args.embedding:\n",
    "        check_embedding()\n",
    "        print(\"Embedding_lookup sanity check passes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae01a84b-76c3-4a80-9cb3-436048537a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
    "    parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
    "    parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
    "    args = parser.parse_args()\n",
    "    \"\"\"\n",
    "    embeddings = np.zeros((100, 30), dtype=np.float32)\n",
    "    model = ParserModel(embeddings)\n",
    "\n",
    "    def check_forward():\n",
    "        inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
    "        out = model(inputs)\n",
    "        expected_out_shape = (4, 3)\n",
    "        assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
    "                                                \" which doesn't match expected \" + repr(expected_out_shape)\n",
    "    if args.forward:\n",
    "        check_forward()\n",
    "        print(\"Forward sanity check passes!\")\n",
    "    else:\n",
    "        print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a7468d-895c-4eb6-a8ce-eddc9cca477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f79127-556f-4826-a1fa-4bda5cf74373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
